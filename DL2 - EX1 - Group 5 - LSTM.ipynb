{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim --upgrade\n",
    "#!pip install numpy --upgrade\n",
    "#!pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import gensim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import gensim.downloader as api\n",
    "from typing import Optional\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doron.z/gensim-data\\glove-wiki-gigaword-300\\glove-wiki-gigaword-300.gz\n"
     ]
    }
   ],
   "source": [
    "# If we want to save the model to a local file\n",
    "glove_path = api.load('glove-wiki-gigaword-300', return_path=True)  \n",
    "print(glove_path)\n",
    "try:\n",
    "    glove_model = gensim.models.keyedvectors.load_word2vec_format(glove_path)\n",
    "except:\n",
    "    glove_model = gensim.models.keyedvectors.load_word2vec_format(glove_path, binary=True)\n",
    "#glove_model = api.load('word2vec-ruscorpora-300')#  If we want to load without saving localy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(400000, 300)\n"
     ]
    }
   ],
   "source": [
    "weights = torch.FloatTensor(glove_model.vectors)\n",
    "embedding = nn.Embedding.from_pretrained(weights)\n",
    "embedding.requires_grad_(False)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_reviews(data_dir,label):\n",
    "    words_lost = 0 #counter of lost words from the reviews\n",
    "    total_words_from_files = 0\n",
    "    all_reviews_array_with_labels = []\n",
    "    #data_dir = r'C:\\Users\\doron.z\\Desktop\\General Stuff\\university\\deep learning 2\\train_data\\train\\neg'\n",
    "    for (roots,dirs,files) in os.walk(data_dir):\n",
    "        for file_dir in files: #for each file in the folder\n",
    "            file = open(roots+f'/{file_dir}','r',encoding='utf-8')\n",
    "            sentences = file.readlines() #each file has 1 sentence/review\n",
    "            sentences = [sen.strip().lower() for sen in sentences]\n",
    "            sentences = [sen.split() for sen in sentences if sen]\n",
    "            #print(sentences)\n",
    "            #print(len(sentences[0]))\n",
    "            representation = []        \n",
    "            for word in sentences[0]: #for every word in the review from the file\n",
    "\n",
    "                if word not in glove_model.key_to_index:\n",
    "                    words_lost +=1\n",
    "                    total_words_from_files+=1\n",
    "                    continue\n",
    "                else:\n",
    "                    word_id = torch.tensor(glove_model.key_to_index[word])  # ID of the word in the embedding\n",
    "                    total_words_from_files+=1\n",
    "\n",
    "                vec = glove_model[word] \n",
    "                representation.append(vec)\n",
    "                #print(torch.equal(torch.Tensor(vec),embedding(word_id)))\n",
    "                \n",
    "            representation = np.asarray(representation) #np array of the embedded words of the sentence\n",
    "            # the shape of the representation is (#of words in the sentence,length of the embedding vector=300)\n",
    "            #print(representation.shape)\n",
    "            \n",
    "            all_reviews_array_with_labels.append(representation)\n",
    "        all_reviews_array_with_labels = np.asarray(all_reviews_array_with_labels)\n",
    "    return all_reviews_array_with_labels,words_lost,total_words_from_files\n",
    "            #print(\"words_lost = \"+str(words_lost))\n",
    "            \n",
    "            #print(representation[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18750,)\n",
      "[[-0.20664   -0.1377    -0.11119   ...  0.27927   -0.0069871  0.13851  ]\n",
      " [-0.076947  -0.021211   0.21271   ...  0.18351   -0.29183   -0.046533 ]\n",
      " [-0.29712    0.094049  -0.096662  ...  0.059717  -0.22853    0.29602  ]\n",
      " ...\n",
      " [-0.51102    0.61752   -0.35497   ... -0.71145   -0.17716   -0.18386  ]\n",
      " [-0.33848    0.42841   -0.10284   ... -0.79888   -0.41967   -0.14039  ]\n",
      " [-0.094833   0.24367    0.18525   ... -0.11331   -0.047248  -0.11424  ]]\n",
      "(100, 300)\n",
      "The number of words dropped from the negative reviews is- 709154 which is 16.52% of all words in from reviews.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doron.z\\AppData\\Local\\Temp/ipykernel_13708/1765626913.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  all_reviews_array_with_labels = np.asarray(all_reviews_array_with_labels)\n"
     ]
    }
   ],
   "source": [
    "neg_reviews,neg_words_lost,neg_total_words_from_files = read_reviews(r'C:\\Users\\doron.z\\Desktop\\General Stuff\\university\\deep learning 2\\train_data\\train\\neg',0)  \n",
    "print(neg_reviews.shape)\n",
    "print(neg_reviews[0])\n",
    "print(neg_reviews[0].shape)\n",
    "print(\"The number of words dropped from the negative reviews is- \"+str(neg_words_lost)+\" which is \"+str(format(neg_words_lost/neg_total_words_from_files*100,\".2f\"))+\"% of all words in from reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18750,)\n",
      "[[-0.62976    0.26793    0.1808    ...  0.32447    0.37457   -0.3564   ]\n",
      " [-0.33112    0.53214    0.22707   ... -0.82863   -0.24082    0.0065358]\n",
      " [-0.1749     0.22956    0.24924   ... -0.24131   -0.40402    0.054744 ]\n",
      " ...\n",
      " [ 0.035287  -0.11865    0.28912   ...  0.071298   0.34976   -0.1023   ]\n",
      " [-0.18256    0.49851   -0.1639    ... -0.27224   -0.19107   -0.094104 ]\n",
      " [ 0.033284  -0.040754  -0.048377  ... -0.15408    0.17806   -0.19683  ]]\n",
      "(118, 300)\n",
      "The number of words dropped from the positive reviews is- 702107 which is 15.99% of all words from those reviews.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doron.z\\AppData\\Local\\Temp/ipykernel_13708/1765626913.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  all_reviews_array_with_labels = np.asarray(all_reviews_array_with_labels)\n"
     ]
    }
   ],
   "source": [
    "pos_reviews,pos_words_lost,pos_total_words_from_files = read_reviews(r'C:\\Users\\doron.z\\Desktop\\General Stuff\\university\\deep learning 2\\train_data\\train\\pos',1)  \n",
    "print(pos_reviews.shape)\n",
    "print(pos_reviews[0])\n",
    "print(pos_reviews[0].shape)\n",
    "print(\"The number of words dropped from the positive reviews is- \"+str(pos_words_lost)+\" which is \"+str(format(pos_words_lost/pos_total_words_from_files*100,\".2f\"))+\"% of all words from those reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0, 1.0, 1.0, ..., 1.0, 1.0, 1.0], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x = np.concatenate((neg_reviews,pos_reviews),axis =0)\n",
    "#print(x.shape)\n",
    "#print(len(neg_reviews))\n",
    "#train_x_neg = neg_reviews\n",
    "#train_y_neg\n",
    "y_neg = np.full_like(neg_reviews,1.0)\n",
    "y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_reviews_array_to_train_val_test(reviews_array,wanted_y_label,split_frac = 0.8):\n",
    "    #y = np.full_like(reviews_array,wanted_y_label,dtype='float32')\n",
    "    \n",
    "    ## split data into training, validation, and test data (x and y)\n",
    "    split_idx = int(len(reviews_array)*split_frac)\n",
    "    train_x, remaining_x = reviews_array[:split_idx], reviews_array[split_idx:]\n",
    "    \n",
    "    #train_y, remaining_y = y[:split_idx], y[split_idx:]\n",
    "    train_y = np.full_like(train_x,wanted_y_label,dtype='float32')\n",
    "    \n",
    "    test_idx = int(len(remaining_x)*0.5)\n",
    "    val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "    #val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "    \n",
    "    val_y = np.full_like(val_x,wanted_y_label,dtype='float32') \n",
    "    test_y = np.full_like(test_x,wanted_y_label,dtype='float32')\n",
    "\n",
    "    # print out the shapes of your resultant feature data\n",
    "    print(\"For reviews with the label:\"+str(wanted_y_label))\n",
    "    print(\"\\t\\t\\tFeature Shapes:\")\n",
    "    print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "          \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "          \"\\nTest set: \\t\\t{}\".format(test_x.shape),\n",
    "          \"\\nwith first object shape: \"+str(train_x[0].shape)#+\n",
    "          #\"\\nwith first object shape: \"+str(train_y.shape)\n",
    "          )\n",
    "    return train_x,train_y, val_x, test_x, val_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For reviews with the label:0.0\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(15000,) \n",
      "Validation set: \t(1875,) \n",
      "Test set: \t\t(1875,) \n",
      "with first object shape: (100, 300)\n",
      "\n",
      "\n",
      "For reviews with the label:1.0\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(15000,) \n",
      "Validation set: \t(1875,) \n",
      "Test set: \t\t(1875,) \n",
      "with first object shape: (118, 300)\n"
     ]
    }
   ],
   "source": [
    "train_neg_x, train_neg_y, val_neg_x, test_neg_x, val_neg_y, test_neg_y = split_reviews_array_to_train_val_test(neg_reviews,0.0)\n",
    "print(\"\\n\")\n",
    "train_pos_x, train_pos_y, val_pos_x, test_pos_x, val_pos_y, test_pos_y = split_reviews_array_to_train_val_test(pos_reviews,1.0)\n",
    "\n",
    "#Merging the neg and pos to the final sets\n",
    "train_x = np.concatenate((train_neg_x,train_pos_x),axis = 0)\n",
    "train_y = np.concatenate((train_neg_y,train_pos_y),axis = 0)\n",
    "val_x = np.concatenate((val_neg_x,val_pos_x),axis = 0)\n",
    "val_y = np.concatenate((val_neg_y,val_pos_y),axis = 0)\n",
    "test_x = np.concatenate((test_neg_x,test_pos_x),axis = 0)\n",
    "test_y = np.concatenate((test_neg_y,test_pos_y),axis = 0)\n",
    "\n",
    "#print(type(train_x[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n",
      "-0.20664\n",
      "(30000,)\n",
      "(3750,)\n",
      "(3750,)\n",
      "(3750,)\n",
      "(3750,)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_x[0][0][0]))\n",
    "print(train_x[0][0][0])\n",
    "print(train_y.shape)\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(100, 300)\n",
      "(30000,)\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0][0].shape) #word\n",
    "print(train_x[0].shape) #sentence\n",
    "print(train_x.shape) # all sentences\n",
    "print(train_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na = train_x[1]\\nprint(a.shape)\\ny = np.zeros(((2155 - a.shape[0]), 300))\\nb = np.concatenate((a,y),axis=0)\\nprint(b.shape)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "a = train_x[1]\n",
    "print(a.shape)\n",
    "y = np.zeros(((2155 - a.shape[0]), 300))\n",
    "b = np.concatenate((a,y),axis=0)\n",
    "print(b.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 300)\n",
      "(250, 300)\n"
     ]
    }
   ],
   "source": [
    "max_sen = 250\n",
    "\n",
    "for i in range(0,train_x.shape[0]):\n",
    "    if train_x[i].shape[0] < max_sen:\n",
    "        #padding = [[0]]* (max_sen - sentence.shape[0])\n",
    "        padding = np.zeros(((max_sen - train_x[i].shape[0]), 300))\n",
    "        train_x[i] = np.concatenate((train_x[i],padding), axis=0)\n",
    "    else:\n",
    "        train_x[i] = train_x[i][:250]\n",
    "\n",
    "        \n",
    "print(train_x[0].shape) #sentence\n",
    "print(train_x[120].shape) #sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 300)\n",
      "(250, 300)\n"
     ]
    }
   ],
   "source": [
    "max_sen = 250\n",
    "\n",
    "for i in range(0,test_x.shape[0]):\n",
    "    if test_x[i].shape[0] < max_sen:\n",
    "        #padding = [[0]]* (max_sen - sentence.shape[0])\n",
    "        padding = np.zeros(((max_sen - test_x[i].shape[0]), 300))\n",
    "        test_x[i] = np.concatenate((test_x[i],padding), axis=0)\n",
    "    else:\n",
    "        test_x[i] = test_x[i][:250]\n",
    "\n",
    "        \n",
    "print(test_x[0].shape) #sentence\n",
    "print(test_x[120].shape) #sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 300)\n",
      "(250, 300)\n"
     ]
    }
   ],
   "source": [
    "max_sen = 250\n",
    "\n",
    "for i in range(0,val_x.shape[0]):\n",
    "    if val_x[i].shape[0] < max_sen:\n",
    "        #padding = [[0]]* (max_sen - sentence.shape[0])\n",
    "        padding = np.zeros(((max_sen - val_x[i].shape[0]), 300))\n",
    "        val_x[i] = np.concatenate((val_x[i],padding), axis=0)\n",
    "    else:\n",
    "        val_x[i] = val_x[i][:250]\n",
    "\n",
    "        \n",
    "print(val_x[0].shape) #sentence\n",
    "print(val_x[120].shape) #sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_padding(dataset,max_sen = 250):\n",
    "    for i in range(0,dataset.shape[0]):\n",
    "        if dataset[i].shape[0] < max_sen:\n",
    "            #padding = [[0]]* (max_sen - sentence.shape[0])\n",
    "            padding = np.zeros(((max_sen - dataset[i].shape[0]), 300))\n",
    "            dataset[i] = np.concatenate((dataset[i],padding), axis=0)\n",
    "        else:\n",
    "            dataset[i] = train_x[i][:250]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = dataset_padding(train_x)\n",
    "#val_x = dataset_padding(val_x)\n",
    "#test_x = dataset_padding(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        #self.length = [np.sum(1 - np.equal(x,0)) for x in X]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #x = self.data[index]\n",
    "        \n",
    "        x = torch.from_numpy(self.data[index])\n",
    "        y = self.target[index]\n",
    "        #x_len = self.length[index]\n",
    "        \n",
    "        return x, y#, x_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor datasets by concatinating Xs and Ys\n",
    "#train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
    "#valid_data = TensorDataset(val_x, torch.from_numpy(val_y))\n",
    "#test_data = TensorDataset(test_x, torch.from_numpy(test_y))\n",
    "#print(train_data.tensors)\n",
    "\n",
    "train_dataset = MyDataSet(train_x, train_y)\n",
    "val_dataset = MyDataSet(val_x, val_y)\n",
    "test_dataset = MyDataSet(test_x, test_y)\n",
    "\n",
    "BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2066, -0.1377, -0.1112,  ...,  0.2793, -0.0070,  0.1385],\n",
       "         [-0.0769, -0.0212,  0.2127,  ...,  0.1835, -0.2918, -0.0465],\n",
       "         [-0.2971,  0.0940, -0.0967,  ...,  0.0597, -0.2285,  0.2960],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " 0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = int(val_x.shape[0]), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = int(val_x.shape[0]), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 250, 300])\n",
      "torch.Size([5])\n",
      "torch.Size([3750, 250, 300])\n",
      "torch.Size([3750])\n",
      "torch.Size([3750, 250, 300])\n",
      "torch.Size([3750])\n"
     ]
    }
   ],
   "source": [
    "#Check if it worrked?\n",
    "for x,y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break\n",
    "for x,y in val_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break\n",
    "for x,y in test_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveCustomLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        \n",
    "        #i_t\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #f_t\n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_t\n",
    "        self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #o_t\n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    #initializing the weights\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, x, init_states = None):               \n",
    "       # assumes x.shape represents (batch_size, sequence_size, input_size)       \n",
    "        print(\"AAAAAAAAA\")\n",
    "        print(x.size())\n",
    "        print(\"BBBBBBBBBBBB\")\n",
    "        bs, seq_sz,_ = x.size()\n",
    "        print(\"bs=\")\n",
    "        print(bs)\n",
    "        print(\"seq_sz=\")\n",
    "        print(seq_sz)\n",
    "        print(\"x[0]=\")\n",
    "        print(x[0])\n",
    "        print(x[0].size())\n",
    "        hidden_seq = []\n",
    "        \n",
    "        if init_states is None:\n",
    "            h_t, c_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                        )\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "            \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t]\n",
    "            \n",
    "            i_t = torch.sigmoid(x_t * self.U_i + h_t * self.V_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t * self.U_f + h_t * self.V_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t * self.U_c + h_t * self.V_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t * self.U_o + h_t * self.V_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        \n",
    "        #reshape hidden_seq p/ retornar\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "MSELoss()\n"
     ]
    }
   ],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "LSTM_model = NaiveCustomLSTM(input_sz= 250, hidden_sz= 300)\n",
    "optimizer = torch.optim.SGD(LSTM_model.parameters(), lr=0.05)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "print(optimizer)\n",
    "print(loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(inputs, labels, device, model):\n",
    "    # if training on gpu\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "    # zero accumulated gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # get the output from the model\n",
    "    # x.size() -> [batch_size]\n",
    "    batch_size = inputs.size(0)\n",
    "        \n",
    "    # IMPORTANT - change the dimensions of x before it enters the NN, batch size must always be first\n",
    "   # x = inputs.unsqueeze(0)         # x.size() -> [1, batch_size]\n",
    "    #x = x.view(batch_size, -1)      # x.size() -> [batch_size, 1]\n",
    "    predictions = model(x)\n",
    "\n",
    "    loss = loss_func(predictions.squeeze(), labels.float())\n",
    "    \n",
    "    return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, train_loader, model, optimizer):\n",
    "    model.to(device)\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss, predictions = calc_loss(inputs, labels, device, model)\n",
    "            # Try not clipping\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # Doing the optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Actually training the net\n",
    "            \n",
    "            # Print weights\n",
    "            #print(model[0].weight)\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                fig, mean_loss = plot_predictions(valid_loader, model)\n",
    "                print(f\"Epoch: {e + 1}/{epochs}...\\\n",
    "                        \\nStep: {counter}...\\\n",
    "                        \\nVal Loss: {mean_loss}\")         \n",
    "                fig.show()\n",
    "            counter += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(valid_loader ,model):\n",
    "    val_losses = []\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    # Get validation loss\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        order = np.argsort(inputs[:, 0])\n",
    "        sorted_inputs = inputs[order]\n",
    "        sorted_labels = labels[np.argsort(labels)]\n",
    "        val_loss, val_predictions = calc_loss(inputs, labels, device, model)\n",
    "                \n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        # plot and show learning process\n",
    "        ax.cla()\n",
    "        ax.scatter(sorted_inputs[:,0].cpu().data.numpy(), sorted_labels.cpu().data.numpy())\n",
    "        ax.plot(sorted_inputs[:,0].cpu().data.numpy(), val_predictions[order].cpu().data.numpy().squeeze(), 'r-')\n",
    "        plt.pause(0.1)\n",
    "    ax.text(0.5, 0, 'Loss=%.4f' % np.mean(val_losses), fontdict={'size': 10, 'color':  'red'})\n",
    "    return fig, np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training params\n",
    "epochs = 4\n",
    "counter = 0\n",
    "print_every = 2500\n",
    "clip = 1000 # gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAAAAAAA\n",
      "torch.Size([3750, 250, 300])\n",
      "BBBBBBBBBBBB\n",
      "bs=\n",
      "3750\n",
      "seq_sz=\n",
      "250\n",
      "x[0]=\n",
      "tensor([[-0.1329,  0.1699, -0.1436,  ..., -0.2378,  0.1477,  0.6290],\n",
      "        [-0.0983,  0.5568,  0.5399,  ...,  0.2752, -0.1221,  0.0906],\n",
      "        [-0.2044,  0.1643,  0.0418,  ..., -0.3401, -0.0771, -0.0841],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([250, 300])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3750) must match the size of tensor b (250) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13708/1833576648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13708/1068346247.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(epochs, train_loader, model, optimizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[1;31m# Try not clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13708/2995086176.py\u001b[0m in \u001b[0;36mcalc_loss\u001b[1;34m(inputs, labels, device, model)\u001b[0m\n\u001b[0;32m     13\u001b[0m    \u001b[1;31m# x = inputs.unsqueeze(0)         # x.size() -> [1, batch_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#x = x.view(batch_size, -1)      # x.size() -> [batch_size, 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13708/872066668.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, init_states)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mx_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mi_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mU_i\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_t\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV_i\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mf_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mU_f\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_t\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV_f\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mg_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mU_c\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_t\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV_c\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3750) must match the size of tensor b (250) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "train_model(epochs, train_loader, LSTM_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
